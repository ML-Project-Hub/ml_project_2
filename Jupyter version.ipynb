{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Python 2.7.13\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "syn_input_data = np.genfromtxt('Info/input.csv', delimiter=',')\n",
    "syn_output_data = np.genfromtxt('Info/output.csv', delimiter=',').reshape([-1, 1])\n",
    "letor_input_data = np.genfromtxt('Info/Querylevelnorm_X.csv', delimiter=',')\n",
    "letor_output_data = np.genfromtxt('Info/Querylevelnorm_t.csv', delimiter=',').reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(input_data):\n",
    "    indices = [i for i in range(len(input_data))]\n",
    "    random.shuffle(indices)\n",
    "    return indices\n",
    "    \n",
    "def split_data(input_data,indices):\n",
    "    length = len(indices)\n",
    "    training_data = [input_data[indices[i]] for i in range(int(length*0.8))]\n",
    "    validation_data = [input_data[indices[i]] for i in range(int(length*0.8),int(length*0.9))]\n",
    "    test_data = [input_data[indices[i]] for i in range(int(length*0.9),length)]\n",
    "    total_data = [input_data[indices[i]] for i in range(length)]\n",
    "    return np.array(training_data),np.array(validation_data),np.array(test_data),np.array(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For setting hyperparameters muj and sigmaj with respect to M - number of basis functions\n",
    "def k_means(k,input_data):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=None, precompute_distances=True, n_init=30).fit(input_data)\n",
    "    dic = {}\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    for i in range(len(labels)):\n",
    "        try:\n",
    "            dic[labels[i]] += [input_data[i]]\n",
    "        except:\n",
    "            dic[labels[i]] = [input_data[i]]\n",
    "\n",
    "    for i in dic.keys():\n",
    "        cluster_members = np.matrix(dic[i])\n",
    "        dic[i] = {}\n",
    "        dic[i][\"val\"] = cluster_members\n",
    "        dic[i][\"center\"] = list(centers[i])\n",
    "        dic[i][\"spread\"] = np.linalg.pinv(np.matrix(np.cov(cluster_members.T)))\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Given in ppt\n",
    "def compute_design_matrix(X, centers, spreads):\n",
    "    # use broadcast\n",
    "    basis_func_outputs = np.exp(np.sum(np.matmul(X - centers, spreads) * (X - centers),axis=2)/(-2)).T\n",
    "    # insert ones to the 1st col\n",
    "    return np.insert(basis_func_outputs, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def design_mat_gen(total_data, m_value):\n",
    "    dic = k_means(m_value-1,total_data)\n",
    "    centers = [dic[i][\"center\"] for i in dic.keys()]\n",
    "    centers = np.array([list(i) for i in centers])\n",
    "    spreads = [dic[i][\"spread\"].tolist() for i in dic.keys()]\n",
    "    spreads = np.array(spreads)\n",
    "    design_matrix = compute_design_matrix(np.array(total_data[np.newaxis, :, :]),np.array(centers[:, np.newaxis, :]),spreads)\n",
    "    return design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Given in ppt\n",
    "def closed_form_sol(L2_lambda, design_matrix, output_data):\n",
    "    return np.array(np.linalg.solve(\n",
    "    L2_lambda * np.identity(design_matrix.shape[1]) +\n",
    "    np.matmul(design_matrix.T, design_matrix),\n",
    "    np.matmul(design_matrix.T, output_data)\n",
    "    ).flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Given in ppt\n",
    "def SGD_sol(learning_rate,minibatch_size,num_epochs,L2_lambda,design_matrix,output_data):\n",
    "    N, _ = design_matrix.shape\n",
    "    # You can try different mini-batch size size\n",
    "    # Using minibatch_size = N is equivalent to standard gradient descent\n",
    "    # Using minibatch_size = 1 is equivalent to stochastic gradient descent\n",
    "    # In this case, minibatch_size = N is better\n",
    "    early_stop_error = 1\n",
    "    weights = np.zeros([1, len(design_matrix[0])])\n",
    "    # The more epochs the higher training accuracy. When set to 1000000,\n",
    "    # weights will be very close to closed_form_weights. But this is unnecessary\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(N / minibatch_size):\n",
    "            lower_bound = i * minibatch_size\n",
    "            upper_bound = min((i+1)*minibatch_size, N)\n",
    "            Phi = design_matrix[lower_bound : upper_bound, :]\n",
    "            t = output_data[lower_bound : upper_bound, :]\n",
    "            E_D = np.matmul((np.matmul(Phi, weights.T)-t).T,Phi)\n",
    "            E = (E_D + L2_lambda * weights) / minibatch_size\n",
    "            weights = weights - learning_rate * E\n",
    "            if(epoch == 0):\n",
    "                prev = np.linalg.norm(E)\n",
    "                now = np.linalg.norm(E)\n",
    "            else:\n",
    "                prev = now\n",
    "                now = np.linalg.norm(E)\n",
    "                early_stop_error = prev - now\n",
    "                \n",
    "        #Early Stopping\n",
    "        if (early_stop_error < 0.0001):\n",
    "            break\n",
    "                \n",
    "            \n",
    "    return weights.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(output_data,w,dm,lambda_val):\n",
    "    error = 1/2.0*sum([(output_data[i]-sum(w.T*dm[i]))**2 for i in range(len(output_data))]).tolist()[0] + lambda_val*sum(1/2.0*w.T*w)\n",
    "    return error\n",
    "\n",
    "def rms_error(error,test_data_len):\n",
    "    rms = np.sqrt(2.0*error/test_data_len)\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Synthetic or LeToR data\n",
    "##################################\n",
    "###Change syn to letor here####\n",
    "is_syn = True\n",
    "\n",
    "if(is_syn):\n",
    "    input_data = syn_input_data\n",
    "    output_data = syn_output_data\n",
    "else:\n",
    "    input_data = letor_input_data\n",
    "    output_data = letor_output_data\n",
    "##################################\n",
    "\n",
    "\n",
    "indices = shuffle(input_data)\n",
    "mat1,mat2,mat3,tot_in = split_data(input_data,indices)\n",
    "out1,out2,out3,tot_out = split_data(output_data,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLOSED FORM FOR SYNTHETIC DATA\n"
     ]
    }
   ],
   "source": [
    "if(len(tot_in.T) == 10):\n",
    "    print \"CLOSED FORM FOR SYNTHETIC DATA\"\n",
    "elif (len(tot_in.T) == 46):\n",
    "    print \"CLOSED FORM FOR LETOR DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lambda Hyperparameter\n",
    "lambda_value = 0.1\n",
    "m = 2 #incremented every turn\n",
    "\n",
    "\n",
    "p = 6\n",
    "j = 0\n",
    "\n",
    "\n",
    "#Initial Minimum error (infinity) found during training validation cycle\n",
    "min_err_val = np.inf\n",
    "\n",
    "while (j + 2 < p):\n",
    "    #compute design matrix for hyper parameter M = m\n",
    "    design_matrix = design_mat_gen(tot_in,m)\n",
    "    \n",
    "    #split into training and validation\n",
    "    design_mat_training = design_matrix[:int(len(design_matrix)*0.8)]\n",
    "    design_mat_validation = design_matrix[int(len(design_matrix)*0.8):int(len(design_matrix)*0.9)]\n",
    "    \n",
    "    #compute min w by the closed form solution for that particular lambda and M\n",
    "    w = closed_form_sol(lambda_value,design_mat_training,out1)\n",
    "    \n",
    "    #MSE plus the regularization error\n",
    "    error_min = error(out2,w,design_mat_validation,lambda_value)\n",
    "    \n",
    "    #saving min error state\n",
    "    if (error_min < min_err_val):\n",
    "        j = 0\n",
    "        min_err_val = error_min\n",
    "        min_m = m\n",
    "        min_dm = design_matrix\n",
    "        min_w = w\n",
    "    else:\n",
    "        j +=1\n",
    "    m += 1\n",
    "print \"Optimal M:\", min_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mat2_dm = min_dm[int(len(design_matrix)*0.8):int(len(design_matrix)*0.9)]\n",
    "mat3_dm = min_dm[int(len(design_matrix)*0.9):]\n",
    "\n",
    "print \"Minimum error found in Validation set: \", min_err_val\n",
    "print \"Minimum RMS error found in Validation set: \", rms_error(min_err_val,len(mat2_dm))\n",
    "\n",
    "error_test = rms_error(error(out3,min_w,mat3_dm,lambda_value),len(mat3_dm))\n",
    "\n",
    "print \"RMS in test data is: \", error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Min w:\"\n",
    "print min_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Design Matrix corresponding to min W:\"\n",
    "print min_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = np.array([i[0] for i in np.matmul(np.matrix(mat3_dm.tolist()),np.matrix(min_w.tolist()).T).tolist()])\n",
    "print \"Predicted Y:\"\n",
    "print predicted_y\n",
    "print \"Original Y:\"\n",
    "print out3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_y = [round(i,0) for i in predicted_y]\n",
    "count = 0\n",
    "for i in range(len(scaled_y)):\n",
    "    if out3[i] == scaled_y[i]:\n",
    "        count+=1\n",
    "        \n",
    "print \"Number of records matching in test data after rounding to the nearest decimal:\",count,\n",
    "print \"out of\", len(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(tot_in.T) == 10):\n",
    "    print \"SGD FOR SYNTHETIC DATA\"\n",
    "elif (len(tot_in.T) == 46):\n",
    "    print \"SGD FOR LETOR DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lambda_value = 0.1\n",
    "learn_rate = 0.01\n",
    "m = 2 #incremented every turn\n",
    "\n",
    "\n",
    "p = 6\n",
    "j = 0\n",
    "\n",
    "\n",
    "#Initial Minimum error (infinity) found during training validation cycle\n",
    "min_err_val = np.inf\n",
    "\n",
    "while (j + 2 < p):\n",
    "    #compute design matrix for hyper parameter M = m\n",
    "    design_matrix = design_mat_gen(tot_in,m)\n",
    "    N, D = mat1.shape\n",
    "    #split into training and validation\n",
    "    design_mat_training = design_matrix[:int(len(design_matrix)*0.8)]\n",
    "    design_mat_validation = design_matrix[int(len(design_matrix)*0.8):int(len(design_matrix)*0.9)]\n",
    "    \n",
    "    #Gradient Descent solution for min w\n",
    "    w_sgd = SGD_sol(learning_rate=learn_rate,minibatch_size=N,num_epochs=10000,L2_lambda=lambda_value,design_matrix=design_mat_training,output_data=out1)\n",
    "    w_sgd = np.array(w_sgd.tolist())\n",
    "    \n",
    "    #MSE plus the regularization error\n",
    "    error_min = error(out2,w_sgd,design_mat_validation,lambda_value)\n",
    "    \n",
    "    #saving min error state\n",
    "    if (error_min < min_err_val):\n",
    "        j = 0\n",
    "        min_err_val = error_min\n",
    "        min_m = m\n",
    "        min_dm = design_matrix\n",
    "        min_w = w_sgd\n",
    "    else:\n",
    "        j += 1\n",
    "    m += 1\n",
    "print \"Optimal M:\", min_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2_dm = min_dm[int(len(design_matrix)*0.8):int(len(design_matrix)*0.9)]\n",
    "mat3_dm = min_dm[int(len(design_matrix)*0.9):]\n",
    "\n",
    "print \"Minimum error found in Validation set is: \", min_err_val\n",
    "print \"Minimum RMS error found in Validation set is: \", rms_error(min_err_val,len(mat2_dm))\n",
    "\n",
    "error_test = rms_error(error(out3,min_w,mat3_dm,lambda_value),len(mat3_dm))\n",
    "\n",
    "print \"RMS in test data is: \", error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Min w:\"\n",
    "print min_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = np.array([i[0] for i in np.matmul(np.matrix(mat3_dm.tolist()),np.matrix(min_w.tolist()).T).tolist()])\n",
    "print \"Predicted Y:\"\n",
    "print predicted_y\n",
    "\n",
    "print \"Original Y:\"\n",
    "print out3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_y = [round(i,0) for i in predicted_y]\n",
    "count = 0\n",
    "for i in range(len(scaled_y)):\n",
    "    if out3[i] == scaled_y[i]:\n",
    "        count+=1\n",
    "        \n",
    "print \"Number of records matching in test data after rounding to the nearest decimal:\",count,\n",
    "print \"out of\", len(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
